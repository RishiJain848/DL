{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BatchSize = 8  #BATCHSIZE MAY BE INCREASED FOR FASTER TRAINING\n",
    "\n",
    "trainset = datasets.MNIST(root='./MNIST', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize((8, 8)),\n",
    "                       transforms.ToTensor()]))\n",
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n",
    "                                          shuffle=True, num_workers=4) # Creating dataloader\n",
    "\n",
    "# Validation set with random rotations in the range [-90,90]\n",
    "testset = datasets.MNIST(root='./MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize((8, 8)),\n",
    "                       transforms.ToTensor()]))\n",
    "testLoader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n",
    "                                         shuffle=False, num_workers=4) # Creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples in train set: 60000\n",
      "No. of samples in test set: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Size of train and test datasets\n",
    "print('No. of samples in train set: '+str(len(trainLoader.dataset)))\n",
    "print('No. of samples in test set: '+str(len(testLoader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (fc1): Linear(in_features=64, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (fc4): Linear(in_features=25, out_features=25, bias=True)\n",
      "  (fc5): Linear(in_features=25, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DNN(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(64,50)\n",
    "        self.fc2 = nn.Linear(50,50)\n",
    "        self.fc3 = nn.Linear(50,25)\n",
    "        self.fc4 = nn.Linear(25,25)\n",
    "        self.fc5 = nn.Linear(25,10)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "    \n",
    "net=DNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#parameters = 7985\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "learning_rate=0.020\n",
    "num_epochs=80\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate) #STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 : Training loss: 1.4750 | Training Accuracy: 45.2900\n",
      "Epoch 2/80 : Training loss: 0.4064 | Training Accuracy: 87.3450\n",
      "Epoch 3/80 : Training loss: 0.2747 | Training Accuracy: 91.6167\n",
      "Epoch 4/80 : Training loss: 0.2278 | Training Accuracy: 93.0133\n",
      "Epoch 5/80 : Training loss: 0.1990 | Training Accuracy: 93.9350\n",
      "Epoch 6/80 : Training loss: 0.1762 | Training Accuracy: 94.5517\n",
      "Epoch 7/80 : Training loss: 0.1602 | Training Accuracy: 95.0733\n",
      "Epoch 8/80 : Training loss: 0.1486 | Training Accuracy: 95.3983\n",
      "Epoch 9/80 : Training loss: 0.1400 | Training Accuracy: 95.5933\n",
      "Epoch 10/80 : Training loss: 0.1306 | Training Accuracy: 95.9133\n",
      "Epoch 11/80 : Training loss: 0.1240 | Training Accuracy: 96.1000\n",
      "Epoch 12/80 : Training loss: 0.1189 | Training Accuracy: 96.2500\n",
      "Epoch 13/80 : Training loss: 0.1139 | Training Accuracy: 96.4400\n",
      "Epoch 14/80 : Training loss: 0.1082 | Training Accuracy: 96.5983\n",
      "Epoch 15/80 : Training loss: 0.1039 | Training Accuracy: 96.6883\n",
      "Epoch 16/80 : Training loss: 0.1014 | Training Accuracy: 96.7333\n",
      "Epoch 17/80 : Training loss: 0.0970 | Training Accuracy: 96.9067\n",
      "Epoch 18/80 : Training loss: 0.0947 | Training Accuracy: 97.0150\n",
      "Epoch 19/80 : Training loss: 0.0916 | Training Accuracy: 97.0683\n",
      "Epoch 20/80 : Training loss: 0.0882 | Training Accuracy: 97.2067\n",
      "Epoch 21/80 : Training loss: 0.0869 | Training Accuracy: 97.2400\n",
      "Epoch 22/80 : Training loss: 0.0837 | Training Accuracy: 97.2800\n",
      "Epoch 23/80 : Training loss: 0.0827 | Training Accuracy: 97.2567\n",
      "Epoch 24/80 : Training loss: 0.0802 | Training Accuracy: 97.4450\n",
      "Epoch 25/80 : Training loss: 0.0774 | Training Accuracy: 97.4933\n",
      "Epoch 26/80 : Training loss: 0.0760 | Training Accuracy: 97.5350\n",
      "Epoch 27/80 : Training loss: 0.0740 | Training Accuracy: 97.6917\n",
      "Epoch 28/80 : Training loss: 0.0732 | Training Accuracy: 97.6050\n",
      "Epoch 29/80 : Training loss: 0.0721 | Training Accuracy: 97.6567\n",
      "Epoch 30/80 : Training loss: 0.0703 | Training Accuracy: 97.6883\n",
      "Epoch 31/80 : Training loss: 0.0680 | Training Accuracy: 97.8033\n",
      "Epoch 32/80 : Training loss: 0.0673 | Training Accuracy: 97.7550\n",
      "Epoch 33/80 : Training loss: 0.0670 | Training Accuracy: 97.7883\n",
      "Epoch 34/80 : Training loss: 0.0651 | Training Accuracy: 97.8333\n",
      "Epoch 35/80 : Training loss: 0.0634 | Training Accuracy: 97.9183\n",
      "Epoch 36/80 : Training loss: 0.0627 | Training Accuracy: 97.9100\n",
      "Epoch 37/80 : Training loss: 0.0623 | Training Accuracy: 97.9017\n",
      "Epoch 38/80 : Training loss: 0.0615 | Training Accuracy: 97.9467\n",
      "Epoch 39/80 : Training loss: 0.0600 | Training Accuracy: 97.9817\n",
      "Epoch 40/80 : Training loss: 0.0586 | Training Accuracy: 98.0317\n",
      "Epoch 41/80 : Training loss: 0.0578 | Training Accuracy: 98.0933\n",
      "Epoch 42/80 : Training loss: 0.0557 | Training Accuracy: 98.1333\n",
      "Epoch 43/80 : Training loss: 0.0563 | Training Accuracy: 98.1283\n",
      "Epoch 44/80 : Training loss: 0.0536 | Training Accuracy: 98.2717\n",
      "Epoch 45/80 : Training loss: 0.0533 | Training Accuracy: 98.2400\n",
      "Epoch 46/80 : Training loss: 0.0540 | Training Accuracy: 98.1883\n",
      "Epoch 47/80 : Training loss: 0.0525 | Training Accuracy: 98.2317\n",
      "Epoch 48/80 : Training loss: 0.0505 | Training Accuracy: 98.3533\n",
      "Epoch 49/80 : Training loss: 0.0521 | Training Accuracy: 98.2433\n",
      "Epoch 50/80 : Training loss: 0.0502 | Training Accuracy: 98.3100\n",
      "Epoch 51/80 : Training loss: 0.0495 | Training Accuracy: 98.3500\n",
      "Epoch 52/80 : Training loss: 0.0480 | Training Accuracy: 98.3933\n",
      "Epoch 53/80 : Training loss: 0.0494 | Training Accuracy: 98.3000\n",
      "Epoch 54/80 : Training loss: 0.0486 | Training Accuracy: 98.3367\n",
      "Epoch 55/80 : Training loss: 0.0482 | Training Accuracy: 98.3683\n",
      "Epoch 56/80 : Training loss: 0.0462 | Training Accuracy: 98.4250\n",
      "Epoch 57/80 : Training loss: 0.0452 | Training Accuracy: 98.4683\n",
      "Epoch 58/80 : Training loss: 0.0466 | Training Accuracy: 98.4200\n",
      "Epoch 59/80 : Training loss: 0.0450 | Training Accuracy: 98.4700\n",
      "Epoch 60/80 : Training loss: 0.0443 | Training Accuracy: 98.5017\n",
      "Epoch 61/80 : Training loss: 0.0447 | Training Accuracy: 98.4983\n",
      "Epoch 62/80 : Training loss: 0.0436 | Training Accuracy: 98.5383\n",
      "Epoch 63/80 : Training loss: 0.0426 | Training Accuracy: 98.5417\n",
      "Epoch 64/80 : Training loss: 0.0430 | Training Accuracy: 98.5233\n",
      "Epoch 65/80 : Training loss: 0.0423 | Training Accuracy: 98.5833\n",
      "Epoch 66/80 : Training loss: 0.0408 | Training Accuracy: 98.6400\n",
      "Epoch 67/80 : Training loss: 0.0404 | Training Accuracy: 98.6033\n",
      "Epoch 68/80 : Training loss: 0.0421 | Training Accuracy: 98.5233\n",
      "Epoch 69/80 : Training loss: 0.0409 | Training Accuracy: 98.6400\n",
      "Epoch 70/80 : Training loss: 0.0374 | Training Accuracy: 98.7217\n",
      "Epoch 71/80 : Training loss: 0.0389 | Training Accuracy: 98.6367\n",
      "Epoch 72/80 : Training loss: 0.0405 | Training Accuracy: 98.6583\n",
      "Epoch 73/80 : Training loss: 0.0388 | Training Accuracy: 98.6517\n",
      "Epoch 74/80 : Training loss: 0.0387 | Training Accuracy: 98.6567\n",
      "Epoch 75/80 : Training loss: 0.0381 | Training Accuracy: 98.6783\n",
      "Epoch 76/80 : Training loss: 0.0375 | Training Accuracy: 98.7350\n",
      "Epoch 77/80 : Training loss: 0.0360 | Training Accuracy: 98.7683\n",
      "Epoch 78/80 : Training loss: 0.0380 | Training Accuracy: 98.7267\n",
      "Epoch 79/80 : Training loss: 0.0377 | Training Accuracy: 98.6433\n",
      "Epoch 80/80 : Training loss: 0.0372 | Training Accuracy: 98.7467\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "for epoch in range(num_epochs):\n",
    "                running_loss = 0.0 \n",
    "                running_corr = 0\n",
    "          \n",
    "                for i,data in enumerate(trainLoader):\n",
    "                    inputs,labels = data\n",
    "                  # Initializing model gradients to zero\n",
    "                    net.zero_grad()\n",
    "                    optimizer.zero_grad() \n",
    "                    # Data feed-forward through the network\n",
    "                    outputs = net(inputs.view(inputs.shape[0],-1))\n",
    "                    # Predicted class is the one with maximum probability\n",
    "                    preds = torch.argmax(outputs,dim=1)\n",
    "                    # Finding the loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Accumulating the loss for each batch\n",
    "                    running_loss += loss \n",
    "                    # Accumulate number of correct predictions\n",
    "                    running_corr += torch.sum(preds==labels)    \n",
    "                    \n",
    "                totalLoss = running_loss/(i+1)\n",
    "                # Calculating gradients\n",
    "                \n",
    "\n",
    "                # Updating the model parameters\n",
    "                for f in net.parameters():\n",
    "                    f.data.sub_(f.grad.data * learning_rate)\n",
    "                    \n",
    "                epoch_loss = running_loss.item()/(i+1)   #Total loss for one epoch\n",
    "                epoch_acc = running_corr.item()/60000\n",
    "                \n",
    "                \n",
    "                    \n",
    "                train_loss.append(epoch_loss) #Saving the loss over epochs for plotting the graph\n",
    "                train_acc.append(epoch_acc) #Saving the accuracy over epochs for plotting the graph\n",
    "                  \n",
    "                    \n",
    "                print('Epoch {:.0f}/{:.0f} : Training loss: {:.4f} | Training Accuracy: {:.4f}'.format(epoch+1,num_epochs,epoch_loss,epoch_acc*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy =  97.39\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for data in testLoader:\n",
    "    inputs,labels = data\n",
    "    # Feedforward train data batch through model\n",
    "    output = net(inputs.view(inputs.shape[0],-1)) \n",
    "    # Predicted class is the one with maximum probability\n",
    "    preds = torch.argmax(output,dim=1)\n",
    "    correct_pred += torch.sum(preds==labels)\n",
    "\n",
    "test_accuracy = correct_pred.item()/10000.0\n",
    "print('Testing accuracy = ',test_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "was able to produce only this much accuracy due to limitation of parametres."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize = 32\n",
    "\n",
    "trainset = datasets.MNIST(root='./MNIST', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize((28, 28)),\n",
    "                       transforms.ToTensor()]))\n",
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n",
    "                                          shuffle=True, num_workers=4) \n",
    "\n",
    "testset = datasets.MNIST(root='./MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.Resize((28, 28)),\n",
    "                       transforms.ToTensor()]))\n",
    "testLoader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n",
    "                                         shuffle=False, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples in train set: 60000\n",
      "No. of samples in test set: 10000\n"
     ]
    }
   ],
   "source": [
    "# Size of train and test datasets\n",
    "print('No. of samples in train set: '+str(len(trainLoader.dataset)))\n",
    "print('No. of samples in test set: '+str(len(testLoader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is avaialble!\n"
     ]
    }
   ],
   "source": [
    "throughput=[]\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if use_gpu:\n",
    "    print('GPU is avaialble!')\n",
    "    \n",
    "else :\n",
    "    print(\"Not Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):   # nn.Module --> Base class for all NN modules\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,10)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "    \n",
    "net=MLP()\n",
    "print(net)\n",
    "net=net.cuda()\n",
    "net=net.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start1 = timeit.default_timer()\n",
    "\n",
    "for i,data in enumerate(trainLoader):\n",
    "    inputs,labels = data\n",
    "    if use_gpu:\n",
    "        inputs, labels = inputs.cuda(),labels.cuda() \n",
    "    inputs=inputs.view(inputs.shape[0],-1)\n",
    "    outputs = net(inputs.double())       #FLOAT_64\n",
    "        # Predicted class is the one with maximum probability\n",
    "    preds = torch.argmax(outputs,dim=1)\n",
    "\n",
    "    \n",
    "stop1 = timeit.default_timer()\n",
    "\n",
    "throughput.append(BatchSize/(stop1-start1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):   # nn.Module --> Base class for all NN modules\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,10)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "    \n",
    "net=MLP()\n",
    "\n",
    "net=net.cuda()\n",
    "net=net.float()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 = timeit.default_timer()\n",
    "\n",
    "for i,data in enumerate(trainLoader):\n",
    "    inputs,labels = data\n",
    "    if use_gpu:\n",
    "        inputs, labels = inputs.cuda(),labels.cuda()\n",
    "    inputs=inputs.view(inputs.shape[0],-1)\n",
    "    outputs = net(inputs.float())       #FLOAT_32\n",
    "        # Predicted class is the one with maximum probability\n",
    "    preds = torch.argmax(outputs,dim=1)\n",
    "\n",
    "    \n",
    "stop2 = timeit.default_timer()\n",
    "\n",
    "throughput.append(BatchSize/(stop2-start2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):   # nn.Module --> Base class for all NN modules\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,100)\n",
    "        self.fc2 = nn.Linear(100,10)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "    \n",
    "net=MLP()\n",
    "print(net)\n",
    "net=net.cuda()\n",
    "net=net.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start3 = timeit.default_timer()\n",
    "\n",
    "for i,data in enumerate(trainLoader):\n",
    "    inputs,labels = data\n",
    "    if use_gpu:\n",
    "        inputs, labels = inputs.cuda(),labels.cuda()\n",
    "    inputs=inputs.view(inputs.shape[0],-1)\n",
    "    outputs = net(inputs.half())       #FLOAT_16\n",
    "        # Predicted class is the one with maximum probability\n",
    "    preds = torch.argmax(outputs,dim=1)\n",
    "\n",
    "    \n",
    "stop3 = timeit.default_timer()\n",
    "\n",
    "throughput.append(BatchSize/(stop3-start3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.638978943295713, 7.457781440998589, 7.262451563987096]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x2638e9b8ba8>,\n",
       "  <matplotlib.axis.XTick at 0x2638e9b4390>,\n",
       "  <matplotlib.axis.XTick at 0x2638e9b4860>],\n",
       " <a list of 3 Text xticklabel objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(3)\n",
    "plt.scatter(x,throughput)\n",
    "plt.xticks(x, ['fp64','fp32','fp16'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
